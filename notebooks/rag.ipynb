{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List\n",
    "from jinja2 import Template\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai.chat_models import ChatOpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path='../src/.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_urls(urls_path: str) -> List[str]:\n",
    "    with open(urls_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        urls = [line.strip() for line in file if line.strip()]\n",
    "    return urls\n",
    "\n",
    "\n",
    "def load_documents(urls_list: List[str]) -> List[Document]:\n",
    "    docs = [WebBaseLoader(url).load() for url in urls_list]\n",
    "    docs_list = [item for sublist in docs for item in sublist]\n",
    "    print(f\"len of documents :{len(docs_list)}\")\n",
    "    return docs_list\n",
    "\n",
    "\n",
    "def preprocess_documents(documents: List[Document]) -> List[Document]:\n",
    "    for doc in documents:\n",
    "        doc.page_content = re.sub(r'\\n+', '\\n', doc.page_content.strip())\n",
    "    return documents\n",
    "\n",
    "\n",
    "def split_documents(documents: List[Document], chunk_size: int, chunk_overlap: int) -> List[Document]:\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)\n",
    "    print(f\"length of document chunks generated :{len(doc_splits)}\")\n",
    "    return doc_splits\n",
    "\n",
    "\n",
    "def read_template_file(template_path: str):\n",
    "    with open(template_path, \"r\") as file:\n",
    "        template_content = file.read()\n",
    "\n",
    "    template = Template(template_content).render()\n",
    "    return template\n",
    "\n",
    "\n",
    "def create_prompt_template(template, input_variables: List[str]) -> PromptTemplate:\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=input_variables,\n",
    "    )\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = load_urls('../data/urls.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of documents :8\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_documents = preprocess_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of document chunks generated :156\n"
     ]
    }
   ],
   "source": [
    "splitted_documents = split_documents(preprocessed_documents, chunk_size=1024, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:05<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "embedding_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(\n",
    "    documents=splitted_documents,\n",
    "    embedding=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents = retriever.invoke(\"que necesito para entrar en Tailandia?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = read_template_file('../prompts/router_prompt.jinja')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = create_prompt_template(template, input_variables=['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_router = prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'vectorstore'}\n",
      "The time required to generate response by Router Chain in seconds:0.48729753494262695\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "question = \"Que necesito para visitar Vietnam?\"\n",
    "print(question_router.invoke({\"question\": question}))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"The time required to generate response by Router Chain in seconds:{end - start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
